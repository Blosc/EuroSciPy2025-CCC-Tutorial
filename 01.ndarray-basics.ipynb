{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NDArray: A NDim, Compressed Data Container\n",
    "\n",
    "NDArray objects let users perform different operations with  arrays like setting, copying or slicing them. In this section, we are going to see how to create and manipulate these NDArray arrays, which possess metadata and data. The data is *chunked* and *compressed*; the metadata gives information about the data itself, as well as the chunking and compression. Chunking and compression are features which make NDArray arrays very efficient for working with large data."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-16T08:45:16.168021Z",
     "start_time": "2025-08-16T08:45:16.004696Z"
    }
   },
   "source": [
    "import numpy as np\n",
    "import blosc2"
   ],
   "outputs": [],
   "execution_count": 1
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating an array\n",
    "Let's start by creating a 2D array with 100M elements filled with ``arange``. We can then print out the metadata, which contains information about: the array data (such as ``shape`` and ``dtype``); and how the data is compressed and stored, such as chunk- and block-shapes (``chunks`` and ``blocks``) and compression params (``CParams``). See [here](https://www.blosc.org/python-blosc2/getting_started/overview.html) for an explanation of chunking and blocking.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "shape = (10_000, 10_000)\n",
    "array = blosc2.arange(np.prod(shape), shape=shape)\n",
    "print(array.info)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The ``cratio`` parameter tells us how effective the compression is, since it gives the ratio between the number of bytes required to store the array in uncompressed and compressed form. Here we require almost 500x less space for the compressed array! Note that all the compression and decompression parameters are set to the default, and ``chunks`` and ``blocks`` have been selected automatically - playing around with them will affect the ``cratio`` (as well as compression and decompression speed).\n",
    "\n",
    "We can also create an NDArray by compressing a NumPy array:"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "nparray = np.linspace(0, 100, np.prod(shape), dtype=np.float64).reshape(shape)\n",
    "b2array = blosc2.asarray(nparray)\n",
    "print(b2array.info)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "or an iterator:"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "N = 1000_000\n",
    "rng = np.random.default_rng()\n",
    "it = ((-x + 1, x - 2, rng.normal()) for x in range(N))\n",
    "sa = blosc2.fromiter(it, dtype=\"i4,f4,f8\", shape=(N,))\n",
    "print(sa.info)\n",
    "print(f\"first 3 rows of sa: {sa[:3]}\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "**Exercise:** Create a 2D NDArray with 100M elements filled with sequential integers using the `range` iterator. Then use `blosc2.arange` to create the same array, and check that the two arrays are equal.  Use `%time` magick tool to time the two operations.  What do you notice about the time taken?  Why do you think this is?"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "shape = (1_000, 1_000)\n",
    "%time a = blosc2.fromiter(range(np.prod(shape)), dtype=\"i4\", shape=shape)\n",
    "%time b = blosc2.arange(np.prod(shape), shape=shape)\n",
    "print(np.array_equal(a, b))  # Check that the two arrays are equal"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Reading and modifying data\n",
    "NDArray arrays cannot be read directly, since they are compressed, and so must be decompressed first (to NumPy arrays, which are stored in memory). This can be done for the full array using the ``[:]`` operator, which returns a NumPy array."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "temp = array[:]  # This will decompress the full array\n",
    "type(temp)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "However it is often not necessary (or desirable) to load the whole array into memory. We can easily read just small parts of NDArray arrays to a NumPy array, quickly, via standard indexing routines."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "res1 = array[0]  # get first element\n",
    "res2 = array[6:10]  # get slice\n",
    "print(f\"Got one element (of shape {res1.shape}) and slice of shape {res2.shape}.\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can modify the data in the array using standard NumPy indexing too, using either NumPy or NDArray arrays as the data source.  For example, we can set the first row to zeros (using an NDArray array) and the first column to ones (using a NumPy array)"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "array[0, :] = blosc2.zeros(10000, dtype=array.dtype)\n",
    "array[:, 0] = np.ones(10000, dtype=array.dtype)\n",
    "print(array)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that ``array`` is still an NDArray array. Let's check that the entries were correctly modified."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "print(array[0, 0])\n",
    "print(array[0, :])\n",
    "print(array[:, 0])"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Enlarging the array\n",
    "Existing arrays can be enlarged. This is one operation that is greatly enhanced by the chunking procedure implemented in NDArray arrays."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "array.resize((10_001, 10_000))\n",
    "print(array.shape)\n",
    "array[10_000, :] = 1\n",
    "array[10_000, :]"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Enlarging a NumPy array requires a full copy of the data, since underlying data are stored contiguously in memory, which is very costly: new memory to hold the extended array is allocated, the old data is copied to part of the new memory, and then the new data is written to the remaining new memory.\n",
    "Enlarging is a much faster operation for NDArray arrays because data is chunked, and the chunks may be stored non-contiguously in memory, so one may simply write the necessary new chunks to some arbitrary address in memory and leave the old chunks untouched. The references to the new chunk addresses are then added in the NDArray container, which is a very quick operation.\n",
    "\n",
    "You can also shrink the array."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "array.resize((9_000, 10_000))\n",
    "print(array.shape)\n",
    "print(array[8_999])  # This works\n",
    "# array[9_000]  # This will raise an exception"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Persistent data\n",
    "We can use the `save()` method to store the array on disk.  This is very useful when you are working with a large array but do not need to access it often.\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "array.save(\"array_tutorial.b2nd\", mode=\"w\")  # , contiguous=True)\n",
    "!ls -lh array_tutorial.b2nd"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "For arrays, it is usual to use the `.b2nd` extension. Now let's open the saved array and check that the data saved correctly (decompressing first to be able to compare):"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "array2 = blosc2.open(\"array_tutorial.b2nd\")\n",
    "np.all(array2[:] == array[:])  # Make sure saved array matches original"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In fact it is possible to create a NDArray array directly on disk, specifying where it will be stored, without first creating it in memory. We may also specify the compression/decompression and other storage parameters (e.g ``chunks`` and ``blocks``). For example, a 1000x1000 array filled with the string ``\"pepe\"`` can be created like this:"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "array1 = blosc2.full(\n",
    "    (1000, 1000),\n",
    "    fill_value=b\"pepe\",\n",
    "    chunks=(100, 100),\n",
    "    blocks=(50, 50),\n",
    "    urlpath=\"array1_tutorial.b2nd\",\n",
    "    mode=\"w\",\n",
    ")\n",
    "!ls -lh array1_tutorial.b2nd"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also write direct to disk using the other constructors we saw previously."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "it = ((-x + 1, x - 2, rng.normal()) for x in range(N))\n",
    "sa = blosc2.fromiter(it, dtype=\"i4,f4,f8\", shape=(N,), urlpath=\"sa-1M.b2nd\", mode=\"w\")\n",
    "print(\"3 first rows of sa:\", sa[:3])\n",
    "b2array = blosc2.asarray(nparray, urlpath=\"linspace_array.b2nd\", mode=\"w\")\n",
    "print(\"3 first rows of b2array:\", b2array[:3])"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To delete saved data, one may use the ``remove_urlpath`` method."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "blosc2.remove_urlpath(\"array_tutorial.b2nd\")\n",
    "blosc2.remove_urlpath(\"array1_tutorial.b2nd\")\n",
    "blosc2.remove_urlpath(\"sa-1M.b2nd\")\n",
    "blosc2.remove_urlpath(\"linspace_array.b2nd\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compression params\n",
    "Let's see how to copy the NDArray data whilst altering the compression parameters. This may be useful in many contexts, for example testing how changing the codec of an existing array affects the compression ratio."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "cparams = blosc2.CParams(\n",
    "    codec=blosc2.Codec.LZ4,\n",
    "    clevel=9,\n",
    "    filters=[blosc2.Filter.SHUFFLE],\n",
    "    filters_meta=[0],\n",
    ")\n",
    "\n",
    "array2 = array.copy(chunks=(500, 10_000), blocks=(50, 10_000), cparams=cparams)\n",
    "print(array2.info)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "print(array.info)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "In this case the compression ratio is much higher for the original array, since we have changed to a different codec that is optimised for compression speed, not compression ratio. In general there is a tradeoff between the two."
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "**Exercise**: Do a computation on the two arrays and see how the time taken compares.  For example, you could compute the sum of all the elements in each array with the `sum()` method."
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "%time array_sum = array.sum()\n",
    "%time array2_sum = array2.sum()\n",
    "print(f\"Sum of original array: {array_sum}, sum of new array: {array2_sum}\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Native Blosc2 codecs\n",
    "Blosc2 supports many standard codecs, since there is no one-size-fits-all compression solution - one codec may be perfect for one context, but quite suboptimal in another.\n",
    "* ZLIB codec: uses the DEFLATE algorithm, is standard, and works well for images.\n",
    "* ZSTD codec: similar compression ratio to ZLIB but faster compression/decompression\n",
    "* LZ4 codec: even faster comp/decomp than ZSTD but reduced compression ratio.\n",
    " * BloscLZ: Blosc implementation of the popular LZ algorithms (good for repeated data e.g. text). Similar tradeoff to LZ4.\n",
    "\n",
    "Finally, via package extensions to Blosc2, one may access the JPEG2000 family of compression algorithms, which aim for a compromise between compression ratio and image quality; Blosc2 implements GROK (``blosc2-grok``) and OPENHTJ2K (``blosc2-openhtj2k``)."
   ]
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## TreeStore: Endowing your data with a hierarchical structure\n",
    "With the `TreeStore` class, you can create a hierarchical structure for your data. This is useful when you want to store data in a tree-like format, where each node can have multiple children. The `TreeStore` class allows you to create, read, and modify trees of NDArray arrays.\n",
    "\n",
    "Let's see an example:"
   ]
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-16T08:46:06.904156Z",
     "start_time": "2025-08-16T08:46:06.856796Z"
    }
   },
   "cell_type": "code",
   "source": [
    "with blosc2.TreeStore(\"example_tree.b2z\", mode=\"w\") as tstore:\n",
    "    tstore[\"/data\"] = np.array([1, 2, 3])  # numpy array\n",
    "    tstore[\"/dir1/data1\"] = blosc2.ones((2, 10)) # blosc2 array\n",
    "    tstore[\"/dir1/data2\"] = blosc2.linspace(0, 1, 1e6, shape=(1000, 1000)) # blosc2 array\n",
    "    tstore.vlmeta[\"author\"] = \"blosc2\""
   ],
   "outputs": [],
   "execution_count": 4
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-16T08:18:10.637694Z",
     "start_time": "2025-08-16T08:18:10.628911Z"
    }
   },
   "cell_type": "markdown",
   "source": [
    "This will create a tree structure with a root node and two child nodes:\n",
    "![Alt text](tree-store-example.png)\n",
    "\n",
    "Let's explore the tree structure we just created.  Let's re-open the `TreeStore` and print out a dataset and some metadata."
   ]
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-16T08:46:48.688645Z",
     "start_time": "2025-08-16T08:46:48.682055Z"
    }
   },
   "cell_type": "code",
   "source": [
    "with blosc2.TreeStore(\"example_tree.b2z\", mode=\"r\") as tstore2:\n",
    "    print(\"/dir1/dat12:\\n\", tstore2[\"/dir1/data1\"][:])\n",
    "    print(\"metadata:\", tstore2.vlmeta[:])"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/dir1/dat12:\n",
      " [[1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
      " [1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]]\n",
      "metadata: {'author': 'blosc2'}\n"
     ]
    }
   ],
   "execution_count": 8
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": ""
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
